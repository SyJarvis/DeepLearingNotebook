

深度学习模型有关数值稳定性的典型问题是消失(vanishing)和爆炸(explosion)

梯度消失：...

梯度爆炸：...

**当神经网络的层数较多时，模型的数值稳定性容易变差。**



**随机初始化模型参数**：顾名思义、（权重、损失函数）



PyTorch的默认随机初始化:

```
import torch
torch.nn.init.normal_()
```

作用：使模型net的权重参数采用正态分布的随机初始化方式。



### Xavier随机初始化

还有一种比较常用的随机初始化方法叫作Xavier随机初始化。
假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布

$$
U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).
$$

它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。



#### 考虑环境因素

**协变量偏移**：统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说P（x）改变了，但P（y∣x）保持不变。尽管它的有用性并不局限于此，当我们认为x导致y时，协变量移位通常是正确的假设。(虽同一类但展现的数据两极分化)

**标签偏移**：当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。

**概念偏移**：标签本身的定义发生变化的情况



https://www.boyuai.com/elites/course/cZu18YmweLv10OeV/jupyter/S45g56IgzHOEPGof2m51V